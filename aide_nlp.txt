I Need this :

For sam project I need to use Contextual Recognition and Reolution
-> undestanding natular language and includes both syntactic and semantic-based reasoning.

Text Summarization
-> retains the key points of the collection

Text Categorization
-> is identifying to which category or class a specific document should be placed based on the
contents of the document.

Steps :
    1. text preprocessing
        a. NLTK -> the natural language toolkit
        b. gensim
        c. textblob
        d. spacy

Pre-processing techniques:
    Help cleaning and stadardization of the text, whit help in analytical systems
    1. Tokenization
    2. Tagging
    3. Chunking
    4. Stemming
    5. Lemmatization

    I Need this :

    For sam project I need to use Contextual Recognition and Reolution
    -> undestanding natular language and includes both syntactic and semantic-based reasoning.

    Text Summarization
    -> retains the key points of the collection

    Text Categorization
    -> is identifying to which category or class a specific document should be placed based on the
    contents of the document.

    Steps :
        1. text preprocessing
            a. NLTK -> the natural language toolkit
            b. gensim
            c. textblob
            d. spacy

    Pre-processing techniques:
        Help cleaning and stadardization of the text, whit help in analytical systems
        1. Tokenization
        2. Tagging
        3. Chunking
        4. Stemming
        5. Lemmatization


        TEXT TOKENIZATION

The most popular tokenization techniques include sentence and word tokenization, which
are used to break sown a text corpus into sentences, and each sentence into workds.
Smaller meaningful components called tokens.

Sentence Tokenization

Using framework NLTK to help us : Basic techniques include looking for specific
delimeters between sentencs, such are period (.) or a newline charachter (\n).
  1. sent_tokenize
  2. PunktSentenceTokenizer
  3. RegexpTokenizer
  4. Pre-trained sentence tokenization models

Word TOKENIZATION

Is the process of splitting or sementaing sentences into their constituent workds.
A sentence is a collection of words, and with tokenization we essentially split a sentence into a list of workds
that can be used to reconstruct the sentence.
  1. word_tokenize
  2. TreebankWordTokenizer
  3. RegexpTokenizer
  4. Inherited tokenizers from RegexpTokenizer

  How to use text pattern :
  Besides the base RegexpTokenizer class, there are several derived classes that perfom
  different types of word tokenization. The WordPunktTokenizer uses the pattern
  r'\w+/[^\w\s]+' to tokenize sentences into independente alphabetic and
  non-alphabetic tokens. The WhitespaceTokenizer tokenizes sentences into words
  based on whitespaces like tabs, newlines, and spaces.

Text Normalization

Is defined as a process that consits of a series of steps that should be followed
to wrangle, clean, and standardize textual data into a form that could be
consumed by other NLP and analytics systems and applications as input.
Often tokenization itself also is a part of text normalization, text normalization
is also colled text cleansing or wrangling (cleaning text, case conversion, correcting
spellings, removing stopwords and other unnecessary terms. stemming, and lemmatization.)
.

Cleaning Text

Textual data we want to use or analyze contains a lot of extraneous and unnecessary
tokens and characters that should be removed before performing any furhter operations like
tokenization or other normalization techniques.

Tokenizing Text

We tokenize text before or after removing unnecessary characters and symbols
from data.

Removig Special Characters

In text normalization involves removing unnecessary and special characters.
Create a pattern of cahracheter you whant to remove them ....
PATTERTN = r'[?|$|&|*|%|@|(|)|~]'

clean_sentence = re.sub(PATTERN,r",sentence)

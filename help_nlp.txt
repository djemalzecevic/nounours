I Need this :

For sam project I need to use Contextual Recognition and Resolution
-> undestanding natular language and includes both syntactic and semantic-based reasoning.

Text Summarization
-> retains the key points of the collection

Text Categorization
-> is identifying to which category or class a specific document should be placed based on the
contents of the document.

Steps :
    1. text preprocessing
        a. NLTK -> the natural language toolkit
        b. gensim
        c. textblob
        d. spacy

Pre-processing techniques:
    Help cleaning and stadardization of the text, whit help in analytical systems
    1. Tokenization
    2. Tagging
    3. Chunking
    4. Stemming
    5. Lemmatization

    I Need this :

    For sam project I need to use Contextual Recognition and Resolution
    -> undestanding natular language and includes both syntactic and semantic-based reasoning.

    Text Summarization
    -> retains the key points of the collection

    Text Categorization
    -> is identifying to which category or class a specific document should be placed based on the
    contents of the document.

    Steps :
        1. text preprocessing
            a. NLTK -> the natural language toolkit
            b. gensim
            c. textblob
            d. spacy

    Pre-processing techniques:
        Help cleaning and stadardization of the text, whit help in analytical systems
        1. Tokenization
        2. Tagging
        3. Chunking
        4. Stemming
        5. Lemmatization


        TEXT TOKENIZATION

The most popular tokenization techniques include sentence and word tokenization, which
are used to break sown a text corpus into sentences, and each sentence into workds.
Smaller meaningful components called tokens.

Sentence Tokenization

Using framework NLTK to help us : Basic techniques include looking for specific
delimeters between sentencs, such are period (.) or a newline charachter (\n).
  1. sent_tokenize
  2. PunktSentenceTokenizer
  3. RegexpTokenizer
  4. Pre-trained sentence tokenization models

Word TOKENIZATION

Is the process of splitting or sementaing sentences into their constituent workds.
A sentence is a collection of words, and with tokenization we essentially split a sentence into a list of workds
that can be used to reconstruct the sentence.
  1. word_tokenize
  2. TreebankWordTokenizer
  3. RegexpTokenizer
  4. Inherited tokenizers from RegexpTokenizer

  How to use text pattern :
  Besides the base RegexpTokenizer class, there are several derived classes that perfom
  different types of word tokenization. The WordPunktTokenizer uses the pattern
  r'\w+/[^\w\s]+' to tokenize sentences into independente alphabetic and
  non-alphabetic tokens. The WhitespaceTokenizer tokenizes sentences into words
  based on whitespaces like tabs, newlines, and spaces.

Text Normalization

Is defined as a process that consits of a series of steps that should be followed
to wrangle, clean, and standardize textual data into a form that could be
consumed by other NLP and analytics systems and applications as input.
Often tokenization itself also is a part of text normalization, text normalization
is also colled text cleansing or wrangling (cleaning text, case conversion, correcting
spellings, removing stopwords and other unnecessary terms. stemming, and lemmatization.)
.

Cleaning Text

Textual data we want to use or analyze contains a lot of extraneous and unnecessary
tokens and characters that should be removed before performing any furhter operations like
tokenization or other normalization techniques.

Tokenizing Text

We tokenize text before or after removing unnecessary characters and symbols
from data.

Removig Special Characters

In text normalization involves removing unnecessary and special characters.
Create a pattern of cahracheter you whant to remove them ....
PATTERTN = r'[?|$|&|*|%|@|(|)|~]'

clean_sentence = re.sub(PATTERN,r",sentence)

filtered_list = [remove__characters_before_tokenization(sentence) for sentence in corpus]

Expanding Contractions

Are shortened version of words or syllables. They exist in either written or spoken forms.
Shortened versions of existing words are created by removing specific letters and sounds.
Usually contractions are avoided when used in formal writing, but informally, they are used
quite extensively. Various forms of contractions exist that are tied down to the type of
auxiliary verbs that give us normal contractions, negated contractions, and other special
colloquial contractions.

Exemple :
CONTRACTION_MAP = {
"isn't":"is not",
"aren't":are not"
etc....
}

How to use NLTK lib for stop words

Text Normalization

We will need to normalize our text documents and coorpora as usual before we perform any further analyses or
NLP.

Feature Extraction

To see how to do it........
Exemple  ---> see how to use this lib...

from sklearn.feature_extraction.text import CountVectorize, TfidfVectorize

def build_feature_matrix(documents, feature_type='frequency', ngram_range=(1,1),min_df=0.0,max_df1.0):
    feature_tpye = feature_type.lower().strip()

    if feature_type == 'binary':
      vectorizer = CountVectorize(binary=True, min_df=min_df,max_df=max_df,ngram_range=ngram_range)
    elif feature_type == 'frequency':
      vectorizer = CountVectorize(binary=False, min_df=min_df,max_df=max_df,ngram_range=ngram_range)
    elif feature_tpye == 'tfidf':
      vectorizer = TfidfVectorize(min_df=min_df,max_df=max_df,ngram_range=ngram_range)
    else:
      raise.Exception('Wrong value of feature type, possible value:','binary','frequency','tfidf')

    feature_matrix = vectorizer.fit_transform(documents).astype(float)

    return vectorizer, feature_matrix

This function definition use capabilities for Bag of Words frequency, occurrences, and also TF-IDF-based features.

Text Similarity

The main objective is to analyze and measure how two entities of text are close or far apart from each other.
1. Lexical Similarity -> most popular
  a. term similarity measure similarity between individual token or words
    Analyzing Term Similarity
      - Character vectorization
      - Bag of Characters vectorization
  b. document similarity measuring similarity between entire text documents
2. Semantic similarity

5 Metrics :
  1. Hamming distance
  2. Manhattan distance
  3. Euclidean distance
  4. Levenshtein edit distance
  5. Cosine distance and similarity

Analyzing Document Similarity

from normalization import normalize_corpus
from utils import build_feature_matrix
import numpy as np

# load the toy corpus index
toy_corpus = ['The sky is blue', 'The sky i blue and beautiful', 'Look at the bright blue sky!',
              'Python is a great Programming language', 'Python and Java are popular Programming language',
              'Among Programming language, both Python and Java are the most used in Analytics',
              'The fox is quicker than the lazy dog', 'The dog is smarter than the fox',
              'The dog, fox and cat are good friends']

# load the docs for which we will be measuring similarities
query_doc = ['The fox is definitely smarter than the dog',
             'Java is a static typed programming language unlike Python',
             'I love to relax under the beautiful blue sky!']

#  normalization and extract features form the toy corpus
norm_corpus = normalize_corpus(toy_corpus, lemmatize=True)
